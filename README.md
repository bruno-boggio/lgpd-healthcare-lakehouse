# LGPD Healthcare Lakehouse - Project Summary

## Overview

**Project Name:** LGPD Healthcare Lakehouse  
**Domain:** Healthcare Data Engineering  
**Cloud Platform:** Microsoft Azure  
**Data Platform:** Azure Databricks  
**Orchestration:** Azure Data Factory  
**Architecture:** Medallion (Bronze/Silver/Gold)  
**Compliance:** LGPD (Data Privacy Law)

---

## Project Objectives

1. Build a production-grade data lakehouse for healthcare analytics
2. Implement LGPD compliance with PII data segregation
3. Create automated ETL pipelines with monitoring and alerts
4. Enable secure data access with role-based controls
5. Demonstrate enterprise-level data engineering practices

---

##  Architecture

### Medallion Architecture (3 Layers)

```
Landing (CSV files)
    â†“
Bronze (Raw Delta Lake)
    â†“
Silver (Cleaned & Validated)
    â†“
Gold (Business Logic & Analytics)
```

### Technology Stack

- **Storage:** Azure Data Lake Storage Gen2 (ADLS)
- **Compute:** Azure Databricks (Spark)
- **Orchestration:** Azure Data Factory (ADF)
- **Format:** Delta Lake
- **Catalog:** Hive Metastore
- **Alerting:** Azure Logic Apps
- **Language:** Python, PySpark, SQL

---

## ðŸ“Š Data Model

### Star Schema Design

**Dimensions (6):**
- `dim_data` - Date dimension
- `dim_clinica` - Clinic/healthcare facility
- `dim_medico` - Doctor (pseudonymized with tokens)
- `dim_diagnostico` - Diagnosis (ICD codes)
- `dim_exame` - Medical exams
- `dim_paciente` - Patient (pseudonymized with tokens)

**Fact Table (1):**
- `fato_consultas` - Medical consultations (grain: one row per consultation)

**PII Tables (2) - Separate secured storage:**
- `paciente_identidade` - Patient PII (CPF, name, email, phone)
- `medico_identidade` - Doctor PII (CPF, name, CRM)

### LGPD Compliance Strategy

**Pseudonymization:**
- Dimensions contain only tokens (no direct identifiers)
- PII stored separately in `/pii/` folder with restricted access
- Tokenization maintained for joins between fact/dimensions

**Physical Separation:**
- Regular data: `/bronze/`, `/silver/`, `/gold/`
- Sensitive data: `/bronze/pii/`, `/silver/pii/`
- Access control via Azure RBAC and ACLs

---

##  ETL Pipelines

### Landing â†’ Bronze (Ingestion)

**3 Notebooks:**
1. `01_ingest_dimensions` - Processes 6 dimension tables
2. `02_ingest_pii` - Processes 2 PII tables (separate path)
3. `03_ingest_facts` - Processes fact table

**Features:**
- Dynamic file detection (only processes existing files)
- Delta Lake format with ACID transactions
- Schema evolution enabled (`mergeSchema: true`)
- Audit columns: `ingestion_timestamp`, `ingestion_date`
- File management: moves processed files to `processed/` or `failed/` folders
- Error handling with detailed logging

**ADF Pipelines (3):**
- `01_Ingest_Dimensions_To_Bronze`
- `02_Ingest_Identities_To_Bronze`
- `03_Ingest_Fact_To_Bronze`

**Pipeline Components:**
- Set Variable (generate execution_id)
- Get Metadata (check for files in landing)
- If Condition (process only if files exist)
- Databricks Notebook activities
- Logging notebooks (start/success/failure)
- Web activity (email alerts on failure)

### Bronze â†’ Silver (Transformation)

**3 Notebooks:**
1. `01_transform_dimensions` - Cleans and validates 6 dimensions
2. `02_transform_pii` - Validates PII data
3. `03_transform_facts` - Validates fact table with FK checks

**Transformations Applied:**

**Data Cleaning:**
- Trim whitespace
- Uppercase/lowercase standardization
- Remove special characters
- Clean phone numbers (digits only)
- Clean CPF (11 digits validation)

**Data Validation:**
- Type casting (int, decimal, date)
- Null value handling
- Range validation (year between 1900-2026, etc.)
- FK validation (all foreign keys must exist)
- Business rule validation (plano_cobriu: 0/1, ativo: 0/1)

**Data Quality:**
- Duplicate removal (keep most recent by ingestion_timestamp)
- Invalid record rejection with logging
- Metrics: input count, output count, rejection rate

**Storage Pattern:**
- MERGE (upsert) instead of append
- Prevents duplicates on reprocessing
- Key-based updates

**ADF Pipelines (3):**
- `pl_transform_dimensions_to_silver`
- `pl_transform_pii_to_silver`
- `pl_transform_facts_to_silver`

---

## Logging & Monitoring

### Audit Table

**Location:** `healthcare_bronze.pipeline_execution_log`

**Schema:**
```sql
execution_id STRING          -- Unique GUID per execution
pipeline_name STRING          -- ADF pipeline name
start_time TIMESTAMP         -- Execution start
end_time TIMESTAMP           -- Execution end
status STRING                -- RUNNING/SUCCESS/FAILED
records_processed INT        -- Count of records
error_message STRING         -- Error details if failed
run_date DATE               -- Partition key
executed_by STRING          -- Trigger type (Manual/Schedule)
```

### Logging Notebooks (3)

1. **`log_pipeline_start`**
   - Inserts record with status='RUNNING'
   - Captures execution_id, pipeline_name, executed_by

2. **`log_pipeline_success`**
   - Updates record with status='SUCCESS'
   - Records end_time and records_processed

3. **`log_pipeline_failure`**
   - Updates record with status='FAILED'
   - Captures error_message for troubleshooting

### Email Alerts

**Failure Alerts:**
- Triggered on pipeline failure
- Azure Logic App sends formatted email
- Contains: pipeline name, error message, timestamp, Databricks run URL

**Daily Report:**
- Scheduled pipeline: `pl_send_daily_report`
- Aggregates execution metrics
- Email includes:
  - Total executions
  - Success/failure breakdown
  - Per-pipeline statistics
  - Records processed

---

##  Security & Compliance

### LGPD Implementation

**1. Data Minimization:**
- Only necessary PII collected
- Pseudonymization with tokens in analytical tables

**2. Physical Separation:**
- PII stored in separate folders (`/pii/`)
- Restricted access via RBAC

**3. Access Control (Attempted):**
- Service Principal created for restricted access
- ACLs configured on `/pii/` folders
- Proof of concept for production implementation

**4. Data Retention:**
- Processed files archived by date
- Failed files preserved for investigation

## ðŸ“¦ Project Structure

```
lgpd-healthcare-lakehouse/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â””â”€â”€ create_databases_and_tables.py
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ 01_ingest_dimensions.py
â”‚   â”‚   â”œâ”€â”€ 02_ingest_identities.py
â”‚   â”‚   â””â”€â”€ 03_ingest_facts.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ 01_transform_dimensions.py
â”‚   â”‚   â”œâ”€â”€ 02_transform_identities.py
â”‚   â”‚   â”œâ”€â”€ 03_transform_facts.py
â”‚   â”‚   â””â”€â”€ 04_mask_pii.py                    
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ 01_aggregate_metrics.py           
â”‚   â”‚   â”œâ”€â”€ 02_aggregate_reports.py           
â”‚   â”‚   â””â”€â”€ 03_visualize_dashboard.py         
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ log_pipeline_start.py
â”‚       â”œâ”€â”€ log_pipeline_success.py
â”‚       â”œâ”€â”€ log_pipeline_failure.py
â”‚       â””â”€â”€ generate_daily_report.py
â”œâ”€â”€ adf/
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ Master_Pipeline_Bronze.json
â”‚       â”œâ”€â”€ Master_Pipeline_Silver.json
â”‚       â”œâ”€â”€ Master_Pipeline_Gold.json         
â”‚       â””â”€â”€ pl_send_daily_report.json
â””â”€â”€ README.md
```

### Storage Structure

```
Azure Data Lake Storage Gen2
â”œâ”€â”€ landing/                    # CSV source files
â”‚   â”œâ”€â”€ processed/             # Successfully processed (by date)
â”‚   â””â”€â”€ failed/                # Failed processing (by date)
â”œâ”€â”€ bronze/                    # Raw Delta Lake
â”‚   â”œâ”€â”€ dim_data/
â”‚   â”œâ”€â”€ dim_clinica/
â”‚   â”œâ”€â”€ dim_medico/
â”‚   â”œâ”€â”€ dim_diagnostico/
â”‚   â”œâ”€â”€ dim_exame/
â”‚   â”œâ”€â”€ dim_paciente/
â”‚   â”œâ”€â”€ facts/
â”‚   â”‚   â””â”€â”€ fato_consultas/
â”‚   â”œâ”€â”€ pii/                   # Restricted access
â”‚   â”‚   â”œâ”€â”€ paciente_identidade/
â”‚   â”‚   â””â”€â”€ medico_identidade/
â”‚   â””â”€â”€ control/
â”‚       â””â”€â”€ pipeline_execution_log/
â”œâ”€â”€ silver/                    # Cleaned Delta Lake
â”‚   â”œâ”€â”€ dim_data/
â”‚   â”œâ”€â”€ dim_clinica/
â”‚   â”œâ”€â”€ dim_medico/
â”‚   â”œâ”€â”€ dim_diagnostico/
â”‚   â”œâ”€â”€ dim_exame/
â”‚   â”œâ”€â”€ dim_paciente/
â”‚   â”œâ”€â”€ facts/
â”‚   â”‚   â””â”€â”€ fato_consultas/
â”‚   â””â”€â”€ pii/                   # With masked columns
â”‚       â”œâ”€â”€ paciente_identidade/
â”‚       â””â”€â”€ medico_identidade/
â””â”€â”€ gold/                      # Business logic      
    â”œâ”€â”€ agg_consultas_por_periodo/
    â”œâ”€â”€ agg_consultas_por_medico/
    â”œâ”€â”€ agg_consultas_por_clinica/
    â”œâ”€â”€ agg_consultas_por_diagnostico/
    â”œâ”€â”€ agg_performance_exames/
    â”œâ”€â”€ agg_resumo_especialidade/
    â”œâ”€â”€ agg_perfil_pacientes/
    â”œâ”€â”€ rpt_top_medicos_receita/
    â”œâ”€â”€ rpt_pacientes_alto_gasto/
    â”œâ”€â”€ rpt_medicos_por_clinica/
    â”œâ”€â”€ rpt_historico_pacientes/
    â”œâ”€â”€ rpt_medicos_pacientes_cidade/
    â””â”€â”€ rpt_contato_pacientes_vip/
```

---

### Template Method Pattern
Fixed processing skeleton with variable transformations:
```python
def process_to_silver():
    1. Read from Bronze (fixed)
    2. Transform (varies - Strategy)
    3. Deduplicate (fixed)
    4. Add metadata (fixed)
    5. MERGE to Silver (fixed)
```

## ðŸ”§ Technical Highlights

### Delta Lake Features
- **ACID Transactions:** Guaranteed consistency
- **Time Travel:** Version history for rollback
- **Schema Evolution:** Add columns without breaking
- **MERGE Operations:** Upsert for idempotency

### PySpark Best Practices
- Type hints for functions
- Reusable utility functions
- Error handling with try/except
- Lazy evaluation optimization
- Avoid unnecessary counts (performance)

---

## ðŸ“ˆ Operational Metrics

**Captured Automatically:**
- Pipeline execution count
- Success/failure rates
- Records processed per run
- Processing duration
- Error messages and stack traces

**Available for Analysis:**
- Daily execution trends
- Pipeline reliability metrics
- Data volume growth
- Error patterns

---

## ðŸš€ Deployment

**Current State:** Development/POC
- Manual trigger via ADF
- Single Databricks cluster
- Hive Metastore
  
---

## ðŸ“š Skills Demonstrated

### Data Engineering
âœ… Medallion architecture implementation  
âœ… ETL pipeline design and development  
âœ… Data modeling (star schema)  
âœ… Data quality and validation  
âœ… Incremental processing patterns  

### Azure Cloud
âœ… Azure Data Lake Storage Gen2  
âœ… Azure Databricks (Spark)  
âœ… Azure Data Factory orchestration  
âœ… Azure Logic Apps integration  
âœ… Service Principal authentication  

### Programming
âœ… Python/PySpark  
âœ… SQL (queries and DDL)  
âœ… Design patterns (Strategy, Template Method)  
âœ… Error handling and logging  
âœ… Code organization and modularity  

### Compliance & Governance
âœ… LGPD data protection principles  
âœ… PII identification and segregation  
âœ… Access control design (RBAC, ACLs)  
âœ… Data retention policies  

### DevOps/Operational
âœ… Pipeline monitoring and alerting  
âœ… Email notifications with Logic Apps  
âœ… Execution logging and metrics  
âœ… Daily operational reports  
âœ… Error handling and recovery  

---

## ðŸ’¼ Business Value

**For Healthcare Organizations:**
- Centralized patient data for analytics
- LGPD-compliant data handling
- Automated data pipelines (reduce manual work)
- Audit trail for compliance reporting
- Scalable architecture for growth
- 100% data lineage tracking
- Zero data loss with ACID transactions

