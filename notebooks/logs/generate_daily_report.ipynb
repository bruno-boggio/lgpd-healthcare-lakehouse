{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199862d8-b735-4975-a77d-9d311898175d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Receber parâmetro de data\n",
    "dbutils.widgets.text(\"report_date\", \"\")\n",
    "report_date = dbutils.widgets.get(\"report_date\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Se não passar data, usa hoje\n",
    "if not report_date:\n",
    "    report_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Gerando relatorio para: {report_date}\")\n",
    "\n",
    "# Query nos logs\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE healthcare_bronze\")\n",
    "\n",
    "df_summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_execucoes,\n",
    "    SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as sucessos,\n",
    "    SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as falhas,\n",
    "    SUM(records_processed) as total_registros,\n",
    "    COUNT(DISTINCT pipeline_name) as pipelines_distintas\n",
    "FROM pipeline_execution_log\n",
    "WHERE run_date = '{report_date}'\n",
    "\"\"\")\n",
    "\n",
    "# Converter para dict\n",
    "summary = df_summary.collect()[0].asDict()\n",
    "\n",
    "# Buscar breakdown por pipeline\n",
    "df_breakdown = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    pipeline_name,\n",
    "    COUNT(*) as total_execucoes,\n",
    "    SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as sucessos,\n",
    "    SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as falhas,\n",
    "    SUM(records_processed) as registros_processados\n",
    "FROM pipeline_execution_log\n",
    "WHERE run_date = '{report_date}'\n",
    "GROUP BY pipeline_name\n",
    "ORDER BY pipeline_name\n",
    "\"\"\")\n",
    "\n",
    "# Formatar breakdown como texto com quebras HTML\n",
    "breakdown_text = \"\"\n",
    "for row in df_breakdown.collect():\n",
    "    breakdown_text += f\"• {row.pipeline_name}: {row.total_execucoes} exec ({row.sucessos} sucessos, {row.falhas} falhas) - {row.registros_processados} registros<br>\"\n",
    "\n",
    "if not breakdown_text:\n",
    "    breakdown_text = \"Nenhuma execucao registrada.\"\n",
    "\n",
    "# Buscar detalhes de falhas\n",
    "df_failures = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    pipeline_name,\n",
    "    DATE_FORMAT(start_time, 'dd/MM/yyyy HH:mm:ss') as start_time,\n",
    "    error_message\n",
    "FROM pipeline_execution_log\n",
    "WHERE run_date = '{report_date}'\n",
    "AND status = 'FAILED'\n",
    "ORDER BY start_time DESC\n",
    "\"\"\")\n",
    "\n",
    "failures_list = [row.asDict() for row in df_failures.collect()]\n",
    "\n",
    "# Preparar output para o ADF\n",
    "output = {\n",
    "    \"report_date\": report_date,\n",
    "    \"total_execucoes\": summary['total_execucoes'],\n",
    "    \"sucessos\": summary['sucessos'],\n",
    "    \"falhas\": summary['falhas'],\n",
    "    \"total_registros\": summary['total_registros'],\n",
    "    \"pipelines_distintas\": summary['pipelines_distintas'],\n",
    "    \"breakdown\": breakdown_text,\n",
    "    \"failures\": failures_list\n",
    "}\n",
    "\n",
    "import json\n",
    "result = json.dumps(output)\n",
    "\n",
    "print(\"Relatorio gerado:\")\n",
    "print(result)\n",
    "\n",
    "# Retornar para o ADF\n",
    "dbutils.notebook.exit(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generate_daily_report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
