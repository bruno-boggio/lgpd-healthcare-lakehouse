{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5beaf5-e425-49a8-9d58-9b992c46e6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze to Silver - Dimensões\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7138022f-94dd-4a3b-a7ba-1cbc15601fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, lower, regexp_replace, to_date,\n",
    "    when, coalesce, lit, current_timestamp, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4430d9-1fbe-4c90-a79d-566f75a7582a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "STORAGE_ACCOUNT = \"mystoacc\"\n",
    "BRONZE_PATH = f\"abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "SILVER_PATH = f\"abfss://silver@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "\n",
    "# Catalog e Database\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS healthcare_silver LOCATION 'abfss://silver@mystoacc.dfs.core.windows.net/'\")\n",
    "spark.sql(\"USE healthcare_silver\")\n",
    "\n",
    "# Timestamp de processamento\n",
    "PROCESSING_TIMESTAMP = datetime.now()\n",
    "PROCESSING_DATE = PROCESSING_TIMESTAMP.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Processamento iniciado: {PROCESSING_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3187c601-b5b0-4d16-aa56-a70959c4f674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções de Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89af5d24-3e04-40af-aa6a-d070c8a47f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_silver_metadata(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Adiciona colunas de metadados da Silver\"\"\"\n",
    "    return df.withColumn(\"silver_processed_at\", lit(PROCESSING_TIMESTAMP)) \\\n",
    "             .withColumn(\"silver_processing_date\", lit(PROCESSING_DATE))\n",
    "\n",
    "def remove_duplicates(df: DataFrame, key_columns: list, table_name: str) -> DataFrame:\n",
    "    \"\"\"Remove duplicatas mantendo o registro mais recente\"\"\"\n",
    "    window_spec = Window.partitionBy(key_columns).orderBy(col(\"ingestion_timestamp\").desc())\n",
    "    \n",
    "    df_dedup = df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                 .filter(col(\"row_num\") == 1) \\\n",
    "                 .drop(\"row_num\")\n",
    "    \n",
    "    return df_dedup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee14887a-aeee-46f8-b626-2131440824a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformações por Dimensão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8a99ed-0ed5-4386-bf4d-6ee41b117e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_transformation(table_name: str, input_count: int, output_count: int):\n",
    "    \"\"\"Loga métricas de transformação\"\"\"\n",
    "    rejected = input_count - output_count\n",
    "    rejection_rate = (rejected / input_count * 100) if input_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{table_name}:\")\n",
    "    print(f\"  Input: {input_count} registros\")\n",
    "    print(f\"  Output: {output_count} registros\")\n",
    "    print(f\"  Rejeitados: {rejected} ({rejection_rate:.2f}%)\")\n",
    "\n",
    "# Transformações ajustadas para o schema real\n",
    "def transform_dim_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_data:\n",
    "    - Validar data_completa\n",
    "    - Validar ano/mes/dia numéricos\n",
    "    \"\"\"\n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_data\").cast(\"int\"),\n",
    "        to_date(col(\"data_completa\"), \"yyyy-MM-dd\").alias(\"data_completa\"),\n",
    "        col(\"ano\").cast(\"int\"),\n",
    "        col(\"mes\").cast(\"int\"),\n",
    "        col(\"dia\").cast(\"int\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    # Remover registros com data inválida\n",
    "    df_transformed = df_transformed.filter(\n",
    "        col(\"data_completa\").isNotNull() &\n",
    "        col(\"ano\").isNotNull() &\n",
    "        col(\"mes\").between(1, 12) &\n",
    "        col(\"dia\").between(1, 31)\n",
    "    )\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def transform_dim_clinica(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_clinica:\n",
    "    - Padronizar tipo_clinica e estado (uppercase, trim)\n",
    "    \"\"\"\n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_clinica\").cast(\"int\"),\n",
    "        upper(trim(col(\"tipo_clinica\"))).alias(\"tipo_clinica\"),\n",
    "        upper(trim(col(\"estado\"))).alias(\"estado\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def transform_dim_medico(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_medico:\n",
    "    - Padronizar especialidade e uf_crm (uppercase)\n",
    "    - Validar ativo (0 ou 1)\n",
    "    \"\"\"\n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_medico\").cast(\"int\"),\n",
    "        upper(trim(col(\"especialidade\"))).alias(\"especialidade\"),\n",
    "        upper(trim(col(\"uf_crm\"))).alias(\"uf_crm\"),\n",
    "        coalesce(col(\"ativo\").cast(\"int\"), lit(1)).alias(\"ativo\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    # Ativo só pode ser 0 ou 1\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        \"ativo\",\n",
    "        when(col(\"ativo\").isin([0, 1]), col(\"ativo\")).otherwise(1)\n",
    "    )\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def transform_dim_diagnostico(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_diagnostico:\n",
    "    - Padronizar codigo_cid e descricao_cid (uppercase)\n",
    "    - Limpar codigo_cid (sem espaços)\n",
    "    \"\"\"\n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_diagnostico\").cast(\"int\"),\n",
    "        upper(trim(regexp_replace(col(\"codigo_cid\"), r\"\\s+\", \"\"))).alias(\"codigo_cid\"),\n",
    "        upper(trim(col(\"descricao_cid\"))).alias(\"descricao_cid\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def transform_dim_exame(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_exame:\n",
    "    - Padronizar tipo_exame e categoria_exame (uppercase)\n",
    "    \"\"\"\n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_exame\").cast(\"int\"),\n",
    "        upper(trim(col(\"tipo_exame\"))).alias(\"tipo_exame\"),\n",
    "        upper(trim(col(\"categoria_exame\"))).alias(\"categoria_exame\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def transform_dim_paciente(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformações dim_paciente:\n",
    "    - Validar token (obrigatório)\n",
    "    - Padronizar sexo (M/F/O)\n",
    "    - Validar ano_nascimento (1900-2026)\n",
    "    - Padronizar cidade (uppercase)\n",
    "    \"\"\"\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    df_transformed = df.select(\n",
    "        col(\"sk_paciente\").cast(\"int\"),\n",
    "        trim(col(\"token_paciente\")).alias(\"token_paciente\"),\n",
    "        upper(trim(col(\"sexo\"))).alias(\"sexo_raw\"),\n",
    "        col(\"ano_nascimento\").cast(\"int\"),\n",
    "        upper(trim(col(\"cidade\"))).alias(\"cidade\"),\n",
    "        col(\"data_cadastro\").cast(\"date\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "    \n",
    "    # Padronizar sexo\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        \"sexo\",\n",
    "        when(col(\"sexo_raw\").isin([\"M\", \"MASCULINO\", \"MALE\"]), \"M\")\n",
    "        .when(col(\"sexo_raw\").isin([\"F\", \"FEMININO\", \"FEMALE\"]), \"F\")\n",
    "        .otherwise(\"O\")\n",
    "    ).drop(\"sexo_raw\")\n",
    "    \n",
    "    # Validar ano de nascimento\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        \"ano_nascimento\",\n",
    "        when(\n",
    "            (col(\"ano_nascimento\") >= 1900) & (col(\"ano_nascimento\") <= current_year),\n",
    "            col(\"ano_nascimento\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Token é obrigatório\n",
    "    df_transformed = df_transformed.filter(col(\"token_paciente\").isNotNull())\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32a2875-92af-4e25-b60d-e23ff9b7c6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline de Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4f6278-b86b-480c-95c6-10a9bf360610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_dimension_to_silver(\n",
    "    table_name: str,\n",
    "    transformation_func,\n",
    "    key_column: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo Bronze → Silver para uma dimensão\n",
    "    \n",
    "    Args:\n",
    "        table_name: Nome da tabela\n",
    "        transformation_func: Função de transformação\n",
    "        key_column: Coluna chave para deduplicação e merge\n",
    "    \n",
    "    Returns:\n",
    "        Dict com métricas do processamento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processando: {table_name}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # 1. Ler da Bronze\n",
    "        bronze_path = f\"{BRONZE_PATH}/{table_name}\"\n",
    "        df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "        \n",
    "        input_count = df_bronze.count()\n",
    "        print(f\"Lidos da Bronze: {input_count} registros\")\n",
    "        \n",
    "        # 2. Aplicar transformações\n",
    "        df_transformed = transformation_func(df_bronze)\n",
    "        \n",
    "        # 3. Remover duplicatas\n",
    "        df_transformed = remove_duplicates(df_transformed, [key_column], table_name)\n",
    "        \n",
    "        # 4. Adicionar metadados Silver\n",
    "        df_transformed = add_silver_metadata(df_transformed)\n",
    "        \n",
    "        output_count = df_transformed.count()\n",
    "        log_transformation(table_name, input_count, output_count)\n",
    "        \n",
    "        # 5. Escrever na Silver com MERGE (upsert)\n",
    "        silver_path = f\"{SILVER_PATH}/{table_name}\"\n",
    "        \n",
    "        if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "            # Merge incremental\n",
    "            delta_table = DeltaTable.forPath(spark, silver_path)\n",
    "            \n",
    "            delta_table.alias(\"target\").merge(\n",
    "                df_transformed.alias(\"source\"),\n",
    "                f\"target.{key_column} = source.{key_column}\"\n",
    "            ).whenMatchedUpdateAll() \\\n",
    "             .whenNotMatchedInsertAll() \\\n",
    "             .execute()\n",
    "            \n",
    "            print(f\"MERGE executado com sucesso\")\n",
    "            \n",
    "        else:\n",
    "            # Primeira carga\n",
    "            df_transformed.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(f\"healthcare_silver.{table_name}\")\n",
    "            \n",
    "            # Registrar no catalog\n",
    "            spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{silver_path}'\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Tabela criada com sucesso\")\n",
    "        \n",
    "        return {\n",
    "            \"table\": table_name,\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"input_count\": input_count,\n",
    "            \"output_count\": output_count,\n",
    "            \"rejected\": input_count - output_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: {str(e)}\")\n",
    "        return {\n",
    "            \"table\": table_name,\n",
    "            \"status\": \"FAILED\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51e6fbd-4786-4843-b92b-43a4367e7ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8da89c6-91d5-409f-91f3-81f5b073e291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de dimensões\n",
    "dimensions_config = [\n",
    "    {\"table\": \"dim_data\", \"transformation\": transform_dim_data, \"key\": \"sk_data\"},\n",
    "    {\"table\": \"dim_clinica\", \"transformation\": transform_dim_clinica, \"key\": \"sk_clinica\"},\n",
    "    {\"table\": \"dim_medico\", \"transformation\": transform_dim_medico, \"key\": \"sk_medico\"},\n",
    "    {\"table\": \"dim_diagnostico\", \"transformation\": transform_dim_diagnostico, \"key\": \"sk_diagnostico\"},\n",
    "    {\"table\": \"dim_exame\", \"transformation\": transform_dim_exame, \"key\": \"sk_exame\"},\n",
    "    {\"table\": \"dim_paciente\", \"transformation\": transform_dim_paciente, \"key\": \"sk_paciente\"}\n",
    "]\n",
    "\n",
    "# Processar todas as dimensões\n",
    "results = []\n",
    "for config in dimensions_config:\n",
    "    result = process_dimension_to_silver(\n",
    "        table_name=config[\"table\"],\n",
    "        transformation_func=config[\"transformation\"],\n",
    "        key_column=config[\"key\"]\n",
    "    )\n",
    "    results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bba53b1-6542-43f9-9a4e-8f5d836a5a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Sumário de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741bcc99-0d63-404b-9eb8-4180c2bad9ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769436590694}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exibir resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMÁRIO DE EXECUÇÃO - BRONZE TO SILVER\")\n",
    "print(\"=\"*80)\n",
    "display(results_df)\n",
    "\n",
    "# Estatísticas\n",
    "total_tables = len(results)\n",
    "success_count = len([r for r in results if r['status'] == 'SUCCESS'])\n",
    "failed_count = len([r for r in results if r['status'] == 'FAILED'])\n",
    "\n",
    "if success_count > 0:\n",
    "    total_input = sum([r.get('input_count', 0) for r in results if r['status'] == 'SUCCESS'])\n",
    "    total_output = sum([r.get('output_count', 0) for r in results if r['status'] == 'SUCCESS'])\n",
    "    total_rejected = sum([r.get('rejected', 0) for r in results if r['status'] == 'SUCCESS'])\n",
    "    \n",
    "    print(f\"\\nTabelas processadas: {success_count}/{total_tables}\")\n",
    "    print(f\"Total de registros input: {total_input:,}\")\n",
    "    print(f\"Total de registros output: {total_output:,}\")\n",
    "    print(f\"Total rejeitado: {total_rejected:,} ({total_rejected/total_input*100:.2f}%)\")\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(f\"\\nALERTA: {failed_count} tabelas falharam!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6664470202266005,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_transform_dimensions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
